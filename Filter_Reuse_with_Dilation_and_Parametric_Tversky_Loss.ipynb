{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__idea__ = 'Filter Sharing And Parametric Tversky Loss Function'\n",
    "__author__ = 'Dawood AL CHANTI'\n",
    "__affiliation__ = 'LS2N-ECN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from os.path import join\n",
    "import random\n",
    "from medpy.io import load\n",
    "from medpy import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.rcParams['image.cmap'] = 'gist_earth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warning in Jupyter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of .py helper module that we import\n",
    "import sys\n",
    "sys.path.insert(1, '/tf/JournalWork/')\n",
    "sys.path.insert(1, '/tf/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_pad(img):    \n",
    "    H,W = img.shape\n",
    "    if H>=512 and W>=512:\n",
    "        PadEdgesSize1_H = int(abs(512-H)/2.)\n",
    "        PadEdgesSize2_H = int(abs(512-H) - PadEdgesSize1_H)\n",
    "        PadEdgesSize1_W = int(abs(512-W)/2.)\n",
    "        PadEdgesSize2_W = int(abs(512-W) - PadEdgesSize1_W)\n",
    "        new=img[PadEdgesSize1_H:H-PadEdgesSize2_H,PadEdgesSize1_W:W-PadEdgesSize2_W]\n",
    "        \n",
    "    elif H<512 and W>=512:\n",
    "        new = np.vstack((img[:,:512],np.zeros_like(img[:,:512])))\n",
    "        HH,WW = new.shape\n",
    "        PadEdgesSize1_H = int(abs(512-HH)/2.)\n",
    "        PadEdgesSize2_H = int(abs(512-HH) - PadEdgesSize1_H)\n",
    "        PadEdgesSize1_W = int(abs(512-WW)/2.)\n",
    "        PadEdgesSize2_W = int(abs(512-WW) - PadEdgesSize1_W)\n",
    "        new=new[:512,PadEdgesSize1_W:WW-PadEdgesSize2_W]\n",
    "    \n",
    "    elif H>=512 and W<512:    \n",
    "        new = np.hstack((img[:512,:],np.zeros_like(img[:512,:])))\n",
    "        HH,WW = new.shape\n",
    "        PadEdgesSize1_H = int(abs(512-HH)/2.)\n",
    "        PadEdgesSize2_H = int(abs(512-HH) - PadEdgesSize1_H)\n",
    "        PadEdgesSize1_W = int(abs(512-WW)/2.)\n",
    "        PadEdgesSize2_W = int(abs(512-WW) - PadEdgesSize1_W)\n",
    "        new=new[PadEdgesSize1_H:HH-PadEdgesSize2_H,:512]\n",
    "    \n",
    "    elif H<512 and W<512:   \n",
    "        new = np.hstack((img[:,:],np.zeros_like(img[:,:])))\n",
    "        new = np.vstack((new[:,:512],np.zeros_like(new[:,:512])))\n",
    "        new = new[:512,:512]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mask(mask_file_path):\n",
    "    '''\n",
    "    Return 3 masks in order SOL, GL and GM.\n",
    "    Input: main path for the dataset: either Train/Val/or Testm i.e. '/tf/volumes/train/CAT_TH/masksX1.mha'\n",
    "    '''\n",
    "    \n",
    "    #Read the data of formate (528, 640, 1574)\n",
    "    mask_data,_ = load(mask_file_path)\n",
    "    # Adjust the formate to (640, 528, 1574)\n",
    "    mask_data = mask_data.transpose([1,0,2])\n",
    "\n",
    "    Mask= list(map(lambda mask_time_step:  crop_pad(mask_time_step),\n",
    "                   mask_data.transpose(2,0,1)))#,(512,640),\n",
    "\n",
    "    # get the output for each muscle of formate timesteps x H x W \n",
    "    # data clip to replace values of 100 150 and 200 to 1\n",
    "    mask_sol= np.array(list(map(lambda mask_time_step: np.clip(np.where(mask_time_step == 100, mask_time_step, 0), \n",
    "                                                               0, 1,\n",
    "                                                               np.where(mask_time_step == 100, mask_time_step, 0))\n",
    "                                ,Mask)))\n",
    "    mask_gl= np.array(list(map(lambda mask_time_step:np.clip(np.where(mask_time_step == 200, mask_time_step, 0), \n",
    "                                                             0, 1,np.where(mask_time_step == 200, mask_time_step, \n",
    "                                                                           0)),Mask)))\n",
    "    mask_gm= np.array(list(map(lambda mask_time_step: np.clip(np.where(mask_time_step == 150, \n",
    "                                                                       mask_time_step, 0), 0, 1,\n",
    "                                                              np.where(mask_time_step == 150, mask_time_step, 0)),\n",
    "                               Mask)))\n",
    "    \n",
    "    mask_sol = np.expand_dims(mask_sol,-1)\n",
    "    mask_gl = np.expand_dims(mask_gl,-1)\n",
    "    mask_gm = np.expand_dims(mask_gm,-1)\n",
    "    \n",
    "    \n",
    "    # return the whole muscles on channel axis of order SOL GL and GM\n",
    "    return np.concatenate([mask_sol,mask_gl,mask_gm],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GL\n",
    "def process_mask_GL(mask_file_path):\n",
    "    '''\n",
    "    Return 3 masks in order SOL, GL and GM.\n",
    "    Input: main path for the dataset: either Train/Val/or Testm i.e. '/tf/volumes/train/CAT_TH/masksX1.mha'\n",
    "    '''\n",
    "    \n",
    "    #Read the data of formate (528, 640, 1574)\n",
    "    mask_data,_ = load(mask_file_path)\n",
    "    # Adjust the formate to (640, 528, 1574)\n",
    "    mask_data = mask_data.transpose([1,0,2])\n",
    "\n",
    "    Mask= list(map(lambda mask_time_step:  crop_pad(mask_time_step),\n",
    "                   mask_data.transpose(2,0,1)))#,(512,640),\n",
    "\n",
    "        \n",
    "    # get the output for each muscle of formate timesteps x H x W \n",
    "    # data clip to replace values of 100 150 and 200 to 1\n",
    "    mask_sol= np.array(list(map(lambda mask_time_step: np.clip(np.where(mask_time_step == 200, mask_time_step, 0), \n",
    "                                                               0, 1,\n",
    "                                                               np.where(mask_time_step == 200, mask_time_step, 0))\n",
    "                                ,Mask)))\n",
    "\n",
    "    mask_sol = np.expand_dims(mask_sol,-1)\n",
    " \n",
    "    # Get the back ground of the annotated mask using the foreground annotation\n",
    "    mask_sol=np.concatenate([mask_sol,1-mask_sol],-1)\n",
    "\n",
    "    return mask_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_file_path):\n",
    "    '''\n",
    "    Return 3 masks in order SOL, GL and GM.\n",
    "    Input: main path for the dataset: either Train/Val/or Testm i.e. '/tf/volumes/train/CAT_TH/masksX1.mha'\n",
    "    '''\n",
    "    \n",
    "    #Read the data of formate (528, 640, 1574)\n",
    "    image_data,_ = load(data_file_path)\n",
    "    # Adjust the formate to (640, 528, 1574)\n",
    "    image_data = image_data.transpose([1,0,2])\n",
    "\n",
    "    image_data= list(map(lambda mask_time_step:  crop_pad(mask_time_step),\n",
    "                   image_data.transpose(2,0,1))) #,(512,640),\n",
    "    \n",
    "    #image_data= list(map(lambda image_data_step: image_data_step[:640,:512],image_data.transpose(2,0,1)))    \n",
    "    \n",
    "    return np.array(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pull_data_from_path(path):\n",
    "    data = process_data(path)\n",
    "    # return normalized data\n",
    "    # values from whole data\n",
    "    mean_val = 19.027262640214904\n",
    "    std_val = 34.175155632916\n",
    "    \n",
    "    data = (data-mean_val) / std_val\n",
    "    # reshape to t,h,w,1\n",
    "    return  np.expand_dims(data,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pull_data_from_path_Complete(path):\n",
    "    data = process_data(path)\n",
    "    # return normalized data\n",
    "    # values from whole data\n",
    "    mean_val = 19.027262640214904\n",
    "    std_val = 34.175155632916\n",
    "    \n",
    "    data = (data-mean_val) / std_val\n",
    "    # reshape to t,h,w,1\n",
    "    return  np.expand_dims(data,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GL\n",
    "def process_mask_GL_Complete(mask_file_path):\n",
    "    '''\n",
    "    Return 3 masks in order SOL, GL and GM.\n",
    "    Input: main path for the dataset: either Train/Val/or Testm i.e. '/tf/volumes/train/CAT_TH/masksX1.mha'\n",
    "    '''\n",
    "    \n",
    "    #Read the data of formate (528, 640, 1574)\n",
    "    mask_data,_ = load(mask_file_path)\n",
    "    # Adjust the formate to (640, 528, 1574)\n",
    "    mask_data = mask_data.transpose([1,0,2])\n",
    "\n",
    "    Mask= list(map(lambda mask_time_step:  crop_pad(mask_time_step),\n",
    "                   mask_data.transpose(2,0,1))) #(512,640)\n",
    "    \n",
    "    # get the output for each muscle of formate timesteps x H x W \n",
    "    # data clip to replace values of 100 150 and 200 to 1\n",
    "    mask_sol= np.array(list(map(lambda mask_time_step: np.clip(np.where(mask_time_step == 200, mask_time_step, 0), \n",
    "                                                               0, 1,\n",
    "                                                               np.where(mask_time_step == 200, mask_time_step, 0))\n",
    "                                ,Mask)))\n",
    "\n",
    "    mask_sol = np.expand_dims(mask_sol,-1)\n",
    " \n",
    "    # Get the back ground of the annotated mask using the foreground annotation\n",
    "    mask_sol=np.concatenate([mask_sol,1-mask_sol],-1)\n",
    "\n",
    "    return mask_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pull_mask_from_path(path):\n",
    "    return process_mask(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Patient_name(diretoryPathforOnePatient):\n",
    "    return diretoryPathforOnePatient.split('/')[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path \n",
    "x_y_path = '/tf/volumes/train/'\n",
    "# define the full path for each patient\n",
    "Patient_folder_path = sorted(os.listdir(x_y_path))\n",
    "Patient_folder_full_path = list(map(lambda v : str(join(x_y_path,v)) + '/', Patient_folder_path))\n",
    "\n",
    "# Get the full path for the volume and the mask\n",
    "DataTrainPath = list(map(lambda s : s+'x1.mha' , Patient_folder_full_path))\n",
    "MasksTrainPath = list(map(lambda s : s+'masksX1.mha' , Patient_folder_full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DataTrainPath[0])\n",
    "print(MasksTrainPath[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the patient Name\n",
    "Patient_name(DataTrainPath[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals()[Patient_name(DataTrainPath[0]) + '_data']=Pull_data_from_path(DataTrainPath[0])\n",
    "print(locals()[Patient_name(DataTrainPath[0]) + '_data'].shape)\n",
    "locals()[Patient_name(DataTrainPath[0]) + '_mask']= process_mask(MasksTrainPath[0])\n",
    "print(locals()[Patient_name(DataTrainPath[0]) + '_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each muscle alone\n",
    "fig, ax = plt.subplots(1,4, sharey=True, figsize=(12,4))\n",
    "\n",
    "ax[0].imshow(locals()[Patient_name(DataTrainPath[0]) + '_data'][560][:,:,0] , aspect=\"auto\",cmap='gray')\n",
    "ax[1].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask'][560][:,:,0] , aspect=\"auto\",cmap='gray')\n",
    "ax[2].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask'][560][:,:,1], aspect=\"auto\",cmap='gray') \n",
    "ax[3].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask'][560][:,:,2] , aspect=\"auto\",cmap='gray') \n",
    "\n",
    "ax[0].set_title('Volume')\n",
    "ax[1].set_title('SOL')\n",
    "ax[2].set_title('GL')\n",
    "ax[3].set_title('GM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for SOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals()[Patient_name(DataTrainPath[0]) + '_mask_GL']= process_mask_GL(MasksTrainPath[0])\n",
    "print(locals()[Patient_name(DataTrainPath[0]) + '_mask_GL'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each muscle alone\n",
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(12,4))\n",
    "\n",
    "ax[0].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask_GL'][700][:,:,0] , aspect=\"auto\",cmap='gray')\n",
    "ax[1].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask_GL'][700][:,:,1] , aspect=\"auto\",cmap='gray')\n",
    "\n",
    "ax[0].set_title('F')\n",
    "ax[1].set_title('B')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data flip from the back to front (as a way of Data Augmentation and mimicing Bi directional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals()[Patient_name(DataTrainPath[0]) + '_data_flip']=np.flip(Pull_data_from_path(DataTrainPath[0]),0)\n",
    "\n",
    "locals()[Patient_name(DataTrainPath[0]) + '_mask_flip']= np.flip(process_mask(MasksTrainPath[0]),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each muscle alone\n",
    "fig, ax = plt.subplots(1,4, sharey=True, figsize=(12,4))\n",
    "\n",
    "ax[0].imshow(locals()[Patient_name(DataTrainPath[0]) + '_data_flip'][700][:,:,0] , aspect=\"auto\",cmap='gray')\n",
    "ax[1].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask_flip'][700][:,:,0] , aspect=\"auto\",cmap='gray')\n",
    "ax[2].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask_flip'][700][:,:,1], aspect=\"auto\",cmap='gray') \n",
    "ax[3].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask_flip'][700][:,:,2] , aspect=\"auto\",cmap='gray') \n",
    "\n",
    "ax[0].set_title('Volume')\n",
    "ax[1].set_title('SOL')\n",
    "ax[2].set_title('GL')\n",
    "ax[3].set_title('GM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each muscle alone\n",
    "fig, ax = plt.subplots(1,4, sharey=True, figsize=(12,4))\n",
    "\n",
    "ax[0].imshow(locals()[Patient_name(DataTrainPath[0]) + '_data'][700][:,:,0] -locals()[Patient_name(DataTrainPath[0]) + '_data_flip'][700][:,:,0], aspect=\"auto\",cmap='gray')\n",
    "ax[1].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask'][700][:,:,0]-locals()[Patient_name(DataTrainPath[0]) + '_mask_flip'][700][:,:,0] , aspect=\"auto\",cmap='gray')\n",
    "ax[2].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask'][700][:,:,1]-locals()[Patient_name(DataTrainPath[0]) + '_mask_flip'][700][:,:,1], aspect=\"auto\",cmap='gray') \n",
    "ax[3].imshow(locals()[Patient_name(DataTrainPath[0]) + '_mask'][700][:,:,2]-locals()[Patient_name(DataTrainPath[0]) + '_mask_flip'][700][:,:,2] , aspect=\"auto\",cmap='gray') \n",
    "\n",
    "ax[0].set_title('Volume')\n",
    "ax[1].set_title('SOL')\n",
    "ax[2].set_title('GL')\n",
    "ax[3].set_title('GM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Memory\n",
    "del(locals()[Patient_name(DataTrainPath[0]) + '_data_flip'])\n",
    "del(locals()[Patient_name(DataTrainPath[0]) + '_data'])\n",
    "del(locals()[Patient_name(DataTrainPath[0]) + '_mask_flip'])\n",
    "del(locals()[Patient_name(DataTrainPath[0]) + '_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Implementation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv3DBlock(inputs, n_filters, kernel_size=[3, 3, 3], stride = [1,1,1],activation_fn=None):\n",
    "        \"\"\"\n",
    "        Builds the 3d conv block \n",
    "        Apply successivly a 3D convolution, BatchNormalization and relu\n",
    "        \"\"\"\n",
    "        # Skip pointwise by setting num_outputs=Non\n",
    "        net = slim.conv3d(inputs, n_filters, kernel_size=kernel_size,stride=stride, activation_fn=activation_fn)\n",
    "        net =  slim.layer_norm(net) #slim.batch_norm(net, fused=True)\n",
    "        net = tf.nn.relu(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv3DBlockTranspose(inputs, n_filters, kernel_size=[3, 3, 3], stride = [1,1,1],activation_fn=None):\n",
    "        \"\"\"\n",
    "        Builds the 3d conv transpose block \n",
    "        Apply successivly a 3D transpose convolution, BatchNormalization and relu\n",
    "        \"\"\"\n",
    "        # Skip pointwise by setting num_outputs=Non\n",
    "        net = slim.conv3d_transpose(inputs, n_filters, kernel_size=kernel_size,\n",
    "                                    stride=stride, activation_fn=activation_fn)\n",
    "        net =  slim.layer_norm(net) #slim.batch_norm(net, fused=True)\n",
    "        net = tf.nn.relu(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AtrousSpatialPyramidPoolingModule_3D(inputs, depth=256):\n",
    "    \n",
    "    '''\n",
    "    5D Tensor: batch, time, H, W, C\n",
    "    '''\n",
    "    feature_map_size = tf.shape(inputs)\n",
    "\n",
    "    # Global average pooling\n",
    "    image_features = tf.reduce_mean(inputs, [2, 3], keep_dims=True)\n",
    "\n",
    "    image_features = slim.conv3d(image_features, depth, [1,1,1], activation_fn=None)\n",
    "\n",
    "    image_features = tf.keras.layers.UpSampling3D(size=(1, int(inputs.shape[2]), \n",
    "                                                        int(inputs.shape[3])))(image_features)\n",
    "    \n",
    "    atrous_pool_block_1 = slim.conv3d(inputs, depth, [1, 1,1], activation_fn=None) # 3x3 filter reciptive field\n",
    "\n",
    "    atrous_pool_block_6 = slim.conv3d(inputs, depth, [3, 3,3], rate=6, activation_fn=None)# 9x9\n",
    "\n",
    "    atrous_pool_block_12 = slim.conv3d(inputs, depth, [3, 3,3], rate=12, activation_fn=None)# 15x15\n",
    "\n",
    "    atrous_pool_block_18 = slim.conv3d(inputs, depth, [3, 3,3], rate=18, activation_fn=None)# 21x21\n",
    "\n",
    "    net = tf.concat((image_features, atrous_pool_block_1, atrous_pool_block_6, \n",
    "                     atrous_pool_block_12, atrous_pool_block_18), axis=-1)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AtrousSpatialPyramidPoolingModule_3D_rate_9(inputs, depth=256):\n",
    "    \n",
    "    '''\n",
    "    5D Tensor: batch, time, H, W, C\n",
    "    '''\n",
    "    feature_map_size = tf.shape(inputs)\n",
    "\n",
    "    # Global average pooling\n",
    "    image_features = tf.reduce_mean(inputs, [2, 3], keep_dims=True)\n",
    "\n",
    "    image_features = slim.conv3d(image_features, depth, [1,1,1], activation_fn=None)\n",
    "  \n",
    "\n",
    "#     image_features = tf.transpose(tf.stack(list(map(lambda a: \n",
    "#                                                     tf.image.resize_bilinear(a, (feature_map_size[2], \n",
    "#                                                                                  feature_map_size[3])), \n",
    "#                                                     tf.unstack(tf.transpose(image_features,[1,0,2,3,4])))),0),\n",
    "#                                   [1,0,2,3,4])\n",
    "    \n",
    "    \n",
    "    image_features = tf.keras.layers.UpSampling3D(size=(1, int(inputs.shape[2]), \n",
    "                                                        int(inputs.shape[3])))(image_features)\n",
    "    \n",
    "    \n",
    "    atrous_pool_block_1 = slim.conv3d(inputs, depth, [1, 1,1], activation_fn=None)# 3x3\n",
    "\n",
    "    atrous_pool_block_6 = slim.conv3d(inputs, depth, [3, 3,3], rate=2, activation_fn=None)#5x5\n",
    "\n",
    "    atrous_pool_block_12 = slim.conv3d(inputs, depth, [3, 3,3], rate=4, activation_fn=None)# 7x7\n",
    "\n",
    "    atrous_pool_block_18 = slim.conv3d(inputs, depth, [3, 3,3], rate=6, activation_fn=None)# 9x9\n",
    "\n",
    "    net = tf.concat((image_features, atrous_pool_block_1, atrous_pool_block_6, \n",
    "                     atrous_pool_block_12, atrous_pool_block_18), axis=-1)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AtrousSpatialPyramidPoolingModule_3D_rate_11(inputs, depth=256):\n",
    "    \n",
    "    '''\n",
    "    5D Tensor: batch, time, H, W, C\n",
    "    '''\n",
    "    feature_map_size = tf.shape(inputs)\n",
    "\n",
    "    # Global average pooling\n",
    "    image_features = tf.reduce_mean(inputs, [2, 3], keep_dims=True)\n",
    "\n",
    "    image_features = slim.conv3d(image_features, depth, [1,1,1], activation_fn=None)\n",
    "  \n",
    "\n",
    "#     image_features = tf.transpose(tf.stack(list(map(lambda a: \n",
    "#                                                     tf.image.resize_bilinear(a, (feature_map_size[2], \n",
    "#                                                                                  feature_map_size[3])), \n",
    "#                                                     tf.unstack(tf.transpose(image_features,[1,0,2,3,4])))),0),\n",
    "#                                   [1,0,2,3,4])\n",
    "    \n",
    "    image_features = tf.keras.layers.UpSampling3D(size=(1, int(inputs.shape[2]), \n",
    "                                                        int(inputs.shape[3])))(image_features)\n",
    "    \n",
    "    \n",
    "    atrous_pool_block_1 = slim.conv3d(inputs, depth, [1, 1,1], activation_fn=None)\n",
    "\n",
    "    atrous_pool_block_6 = slim.conv3d(inputs, depth, [3, 3,3], rate=4, activation_fn=None) #7x7\n",
    "\n",
    "    atrous_pool_block_12 = slim.conv3d(inputs, depth, [3, 3,3], rate=6, activation_fn=None)# 9x9\n",
    "\n",
    "    atrous_pool_block_18 = slim.conv3d(inputs, depth, [3, 3,3], rate=8, activation_fn=None)# 11x11\n",
    "\n",
    "    net = tf.concat((image_features, atrous_pool_block_1, atrous_pool_block_6, \n",
    "                     atrous_pool_block_12, atrous_pool_block_18), axis=-1)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentionRefinementModule_3D(inputs, n_filters):\n",
    "    'for 3d data'\n",
    "    # 3D Global average pooling\n",
    "    net = tf.reduce_mean(inputs, [2, 3], keep_dims=True)\n",
    "    net = slim.conv3d(net, n_filters, kernel_size=[1,1,1])\n",
    "    net = slim.layer_norm(net)\n",
    "    #net = slim.batch_norm(net, fused=True)\n",
    "    net = tf.sigmoid(net)\n",
    "    net = tf.multiply(inputs, net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureFusionModule(input_1, input_2, n_filters):\n",
    "    \n",
    "    inputs = tf.concat([input_1, input_2], axis=-1)\n",
    "    inputs =Conv3DBlock(inputs, n_filters, kernel_size=[3, 3, 3], stride = [1,1,1],activation_fn=None)\n",
    "    \n",
    "\n",
    "    # Global average pooling\n",
    "    net = tf.reduce_mean(inputs, [2, 3], keep_dims=True)\n",
    "    \n",
    "    net = slim.conv3d(net, n_filters, kernel_size=[1,1,1])\n",
    "    net = tf.nn.relu(net)\n",
    "    \n",
    "    net = slim.conv3d(net, n_filters, kernel_size=[1,1,1])\n",
    "    net = tf.sigmoid(net)\n",
    "\n",
    "    net = tf.multiply(inputs, net)\n",
    "\n",
    "    net = tf.add(inputs, net)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureFusionModule_with_Stirde(input_1, input_2, n_filters):\n",
    "    \n",
    "    inputs = tf.concat([input_1, input_2], axis=-1)\n",
    "    inputs =Conv3DBlock(inputs, n_filters, kernel_size=[3, 3, 3], stride = [1,2,2],activation_fn=None)\n",
    "    \n",
    "\n",
    "    # Global average pooling\n",
    "    net = tf.reduce_mean(inputs, [2, 3], keep_dims=True)\n",
    "    \n",
    "    net = slim.conv3d(net, n_filters, kernel_size=[1,1,1])\n",
    "    net = tf.nn.relu(net)\n",
    "    \n",
    "    net = slim.conv3d(net, n_filters, kernel_size=[1,1,1])\n",
    "    net = tf.sigmoid(net)\n",
    "\n",
    "    net = tf.multiply(inputs, net)\n",
    "\n",
    "    net = tf.add(inputs, net)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_coe(output, target, threshold=0.5, axis=(1, 2, 3,4), smooth=1e-5):\n",
    "    \"\"\"Non-differentiable Intersection over Union (IoU) for comparing the similarity \n",
    "    \"\"\"\n",
    "    pre = tf.cast(output > threshold, dtype=tf.float32)\n",
    "    truth = tf.cast(target > threshold, dtype=tf.float32)\n",
    "    inse = tf.reduce_sum(tf.multiply(pre, truth), axis=axis)  # AND\n",
    "    union = tf.reduce_sum(tf.cast(tf.add(pre, truth) >= 1, dtype=tf.float32), axis=axis)  # OR\n",
    "    batch_iou = (inse + smooth) / (union + smooth)\n",
    "    iou = tf.reduce_mean(batch_iou, name='iou_coe')\n",
    "    return iou  # , pre, truth, inse, union\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_hard_coe(output, target, threshold=0.5, axis=(1, 2, 3,4), smooth=1e-5):\n",
    "    \"\"\"Non-differentiable Sørensen–Dice coefficient for comparing the similarity\n",
    "    \"\"\"\n",
    "    output = tf.cast(output > threshold, dtype=tf.float32)\n",
    "    target = tf.cast(target > threshold, dtype=tf.float32)\n",
    "    inse = tf.reduce_sum(tf.multiply(output, target), axis=axis)\n",
    "    l = tf.reduce_sum(output, axis=axis)\n",
    "    r = tf.reduce_sum(target, axis=axis)\n",
    "    hard_dice = (2. * inse + smooth) / (l + r + smooth)\n",
    "    ##\n",
    "    hard_dice = tf.reduce_mean(hard_dice, name='hard_dice')\n",
    "    return hard_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "def confusion(y_true, y_pred):\n",
    "    smooth=1\n",
    "    y_pred_pos = K.clip(y_pred, 0, 1)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.clip(y_true, 0, 1)\n",
    "    y_neg = 1 - y_pos\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg) \n",
    "    prec = (tp + smooth)/(tp+fp+smooth)\n",
    "    recall = (tp+smooth)/(tp+fn+smooth)\n",
    "    return prec, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest the Graph and build it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = None # whatever the depth of the volume, we use a sliding window of T\n",
    "H=512\n",
    "W=512\n",
    "C = 1 # number of input channels\n",
    "num_classes = 2  # or 6 # related to SOl GL and GM\n",
    "n_class = 2 # of 6 if all treated at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the place holder that takes in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input stracture: 5D Tensor\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None,time_step, H, W, C ], name=\"x\")\n",
    "y = tf.placeholder(\"float\", shape=[None,time_step,H, W,num_classes], name=\"y\")\n",
    "\n",
    "# define the placeholder for dropout\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"dropout_probability\")\n",
    "keep_prob_input = tf.placeholder(tf.float32, name=\"dropout_probability_input\")\n",
    "keep_prob_skip = tf.placeholder(dtype=tf.float32,name='SkipDropout')\n",
    "\n",
    "print('Input Structure: ',x)\n",
    "print('Target Structure: ',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_alpha = tf.placeholder(tf.float32, name=\"drop_alpha\")\n",
    "keep_prob_beta = tf.placeholder(tf.float32, name=\"drop_beta\")\n",
    "keep_prob_tp = tf.placeholder(tf.float32, name=\"drop_tp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined as variables ?\n",
    "Batch_size = tf.shape(x)[0]\n",
    "time_size = tf.shape(x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Downsampling_2D(inputs,scale1,scale2):\n",
    "    return tf.image.resize_bilinear(inputs, size=[scale1,scale2])\n",
    "\n",
    "def Down_sample_3D(input_layer,scale1,scale2):\n",
    "    '''\n",
    "    TODO : 10 should be changed to new time scale 10 30 or 200 \n",
    "    '''\n",
    "    unpol_layer = list(map(lambda layer: Downsampling_2D(layer,scale1,scale2), tf.unstack(input_layer, \n",
    "                                                                                  int(input_layer.get_shape()[1]), \n",
    "                                                                                  1)))\n",
    "    return tf.transpose(tf.stack(unpol_layer),[1,0,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip and unpool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip(layer, end_point): return tf.concat([layer, end_point], axis=4)\n",
    "def unpool(layer): return tf.image.resize_nearest_neighbor(layer, \n",
    "                                                           [2*int(layer.get_shape()[1]), \n",
    "                                                            2*int(layer.get_shape()[2])])\n",
    "# unpool based on 2D data\n",
    "def unpool_3D_2(input_layer):\n",
    "    unpol_layer = list(map(lambda layer: unpool(layer), tf.unstack(input_layer, \n",
    "                                                                   int(input_layer.get_shape()[1]), 1)))\n",
    "    return tf.transpose(tf.stack(unpol_layer),[1,0,2,3,4])\n",
    "\n",
    "\n",
    "def unpool_3D(input_layer):\n",
    "    \n",
    "    unpol_layer = tf.keras.layers.UpSampling3D(size=(1, 2,2))(input_layer)\n",
    "    return unpol_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Encoder Target Stream where no annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define for 5D Tensor, here we are dealing with 5D: axis=3 become axis=4 last channel\n",
    "def pixel_wise_softmax(output_map):\n",
    "    return tf.nn.softmax(output_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate=6\n",
    "#strides=[1, 1, 2, 2, 1], \n",
    "Reuse_Layer_1 = False\n",
    "with tf.variable_scope(\"layer_1\",reuse=Reuse_Layer_1) as scope:\n",
    "    \n",
    "    Layer_1 = slim.conv3d(x, 4, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_1 = slim.layer_norm(Layer_1) #slim.batch_norm(net, fused=True)\n",
    "    Layer_1 = tf.nn.relu(Layer_1)\n",
    "    \n",
    "    Layer_1 = tf.nn.max_pool3d(Layer_1, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_1p')\n",
    "    Layer_1 =  tf.nn.dropout(Layer_1,keep_prob_skip)\n",
    "    print(Layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate=6\n",
    "Reuse_Layer_1 = True\n",
    "with tf.variable_scope(\"layer_1\",reuse=Reuse_Layer_1) as scope:\n",
    "    \n",
    "    Layer_1dr2 = slim.conv3d(x, 4, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], rate=3,\n",
    "                          activation_fn=None)\n",
    "    Layer_1dr2 = slim.layer_norm(Layer_1dr2) #slim.batch_norm(net, fused=True)\n",
    "    Layer_1dr2 = tf.nn.relu(Layer_1dr2)\n",
    "    \n",
    "    Layer_1dr2 = tf.nn.max_pool3d(Layer_1dr2, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_1p')\n",
    "    \n",
    "    Layer_1dr2 =  tf.nn.dropout(Layer_1dr2,keep_prob_skip)\n",
    "    print(Layer_1dr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate=6\n",
    "Reuse_Layer_1 = True\n",
    "with tf.variable_scope(\"layer_1\",reuse=Reuse_Layer_1) as scope:\n",
    "    \n",
    "    Layer_1dr3 = slim.conv3d(x, 4, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], rate=7,\n",
    "                          activation_fn=None)\n",
    "    Layer_1dr3 = slim.layer_norm(Layer_1dr3) #slim.batch_norm(net, fused=True)\n",
    "    Layer_1dr3 = tf.nn.relu(Layer_1dr3)\n",
    "    \n",
    "    Layer_1dr3 = tf.nn.max_pool3d(Layer_1dr3, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_1p')\n",
    "    Layer_1dr3 =  tf.nn.dropout(Layer_1dr3,keep_prob_skip)\n",
    "    print(Layer_1dr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate=6\n",
    "Reuse_Layer_1 = True\n",
    "with tf.variable_scope(\"layer_1\",reuse=Reuse_Layer_1) as scope:\n",
    "    \n",
    "    Layer_1dr4 = slim.conv3d(x, 4, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], rate=9,\n",
    "                          activation_fn=None)\n",
    "    Layer_1dr4 = slim.layer_norm(Layer_1dr4) #slim.batch_norm(net, fused=True)\n",
    "    Layer_1dr4 = tf.nn.relu(Layer_1dr4)\n",
    "    \n",
    "    Layer_1dr4 = tf.nn.max_pool3d(Layer_1dr4, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_1p')\n",
    "    Layer_1dr4 =  tf.nn.dropout(Layer_1dr4,keep_prob_skip)\n",
    "    print(Layer_1dr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_1 = tf.concat([tf.concat([tf.concat([Layer_1,Layer_1dr2],-1),Layer_1dr3],-1),Layer_1dr4],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_2 = False\n",
    "with tf.variable_scope(\"layer_2\",reuse=Reuse_Layer_2) as scope:\n",
    "\n",
    "    Layer_2 = slim.conv3d(Layer_1, 8, kernel_size=[3, 3, 3],stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_2 = slim.layer_norm(Layer_2) #slim.batch_norm(net, fused=True)\n",
    "    Layer_2 = tf.nn.relu(Layer_2)\n",
    "    Layer_2 = tf.nn.max_pool3d(Layer_2, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_2p')\n",
    "    Layer_2 =  tf.nn.dropout(Layer_2,keep_prob_skip)\n",
    "    print(Layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_2 = True\n",
    "with tf.variable_scope(\"layer_2\",reuse=Reuse_Layer_2) as scope:\n",
    "    Layer_2r2 = slim.conv3d(Layer_1, 8, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=3,\n",
    "                          activation_fn=None)\n",
    "    Layer_2r2 = slim.layer_norm(Layer_2r2) #slim.batch_norm(net, fused=True)\n",
    "    Layer_2r2 = tf.nn.relu(Layer_2r2)\n",
    "    Layer_2r2 = tf.nn.max_pool3d(Layer_2r2, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_2p')\n",
    "    Layer_2r2 =  tf.nn.dropout(Layer_2r2,keep_prob_skip)\n",
    "    print(Layer_2r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_2 = True\n",
    "with tf.variable_scope(\"layer_2\",reuse=Reuse_Layer_2) as scope:\n",
    "    \n",
    "    Layer_2r3 = slim.conv3d(Layer_1, 8, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=7,\n",
    "                          activation_fn=None)\n",
    "    Layer_2r3 = slim.layer_norm(Layer_2r3) #slim.batch_norm(net, fused=True)\n",
    "    Layer_2r3 = tf.nn.relu(Layer_2r3)\n",
    "    Layer_2r3 = tf.nn.max_pool3d(Layer_2r3, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_2p')\n",
    "    Layer_2r3 =  tf.nn.dropout(Layer_2r3,keep_prob_skip)\n",
    "    print(Layer_2r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_2 = True\n",
    "with tf.variable_scope(\"layer_2\",reuse=Reuse_Layer_2) as scope:\n",
    "\n",
    "    Layer_2r4 = slim.conv3d(Layer_1, 8, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=9,\n",
    "                          activation_fn=None)\n",
    "    Layer_2r4 = slim.layer_norm(Layer_2r4) #slim.batch_norm(net, fused=True)\n",
    "    Layer_2r4 = tf.nn.relu(Layer_2r4)\n",
    "    Layer_2r4 = tf.nn.max_pool3d(Layer_2r4, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_2p')\n",
    "    Layer_2r4 =  tf.nn.dropout(Layer_2r4,keep_prob_skip)\n",
    "    print(Layer_2r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_2 = tf.concat([tf.concat([tf.concat([Layer_2,Layer_2r2],-1),Layer_2r3],-1),Layer_2r4],-1)\n",
    "Layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_3 = False\n",
    "with tf.variable_scope(\"layer_3\",reuse=Reuse_Layer_3) as scope:\n",
    "    Layer_3_2 = slim.conv3d(Layer_2, 16, kernel_size=[3, 3, 3],stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_3_2 = slim.layer_norm(Layer_3_2) #slim.batch_norm(net, fused=True)\n",
    "    Layer_3_2 = tf.nn.relu(Layer_3_2)\n",
    "    Layer_3_2 = tf.nn.max_pool3d(Layer_3_2, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_3p')\n",
    "    Layer_3_2 =  tf.nn.dropout(Layer_3_2,keep_prob_skip)\n",
    "    print(Layer_3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_3 = True\n",
    "with tf.variable_scope(\"layer_3\",reuse=Reuse_Layer_3) as scope:\n",
    "    \n",
    "\n",
    "    Layer_3_2r2 = slim.conv3d(Layer_2, 16, kernel_size=[3, 3, 3],stride=[1, 1, 1],  rate=3,\n",
    "                          activation_fn=None)\n",
    "    Layer_3_2r2 = slim.layer_norm(Layer_3_2r2) #slim.batch_norm(net, fused=True)\n",
    "    Layer_3_2r2 = tf.nn.relu(Layer_3_2r2)\n",
    "    Layer_3_2r2 = tf.nn.max_pool3d(Layer_3_2r2, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_3p')\n",
    "    Layer_3_2r2 =  tf.nn.dropout(Layer_3_2r2,keep_prob_skip)\n",
    "    print(Layer_3_2r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_3 = True\n",
    "with tf.variable_scope(\"layer_3\",reuse=Reuse_Layer_3) as scope:\n",
    "\n",
    "    Layer_3_2r3 = slim.conv3d(Layer_2, 16, kernel_size=[3, 3, 3],stride=[1, 1, 1],  rate=7,\n",
    "                          activation_fn=None)\n",
    "    Layer_3_2r3 = slim.layer_norm(Layer_3_2r3) #slim.batch_norm(net, fused=True)\n",
    "    Layer_3_2r3 = tf.nn.relu(Layer_3_2r3)\n",
    "    Layer_3_2r3 = tf.nn.max_pool3d(Layer_3_2r3, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_3p')\n",
    "    Layer_3_2r3 =  tf.nn.dropout(Layer_3_2r3,keep_prob_skip)\n",
    "    print(Layer_3_2r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_3 = True\n",
    "with tf.variable_scope(\"layer_3\",reuse=Reuse_Layer_3) as scope:\n",
    "    Layer_3_2r4 = slim.conv3d(Layer_2, 16, kernel_size=[3, 3, 3],stride=[1, 1, 1],  rate=9,\n",
    "                          activation_fn=None)\n",
    "    Layer_3_2r4 = slim.layer_norm(Layer_3_2r4) #slim.batch_norm(net, fused=True)\n",
    "    Layer_3_2r4 = tf.nn.relu(Layer_3_2r4)\n",
    "    Layer_3_2r4 = tf.nn.max_pool3d(Layer_3_2r4, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_3p')\n",
    "    Layer_3_2r4 =  tf.nn.dropout(Layer_3_2r4,keep_prob_skip)\n",
    "    print(Layer_3_2r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_3_2 = tf.concat([tf.concat([tf.concat([Layer_3_2,Layer_3_2r2],-1),Layer_3_2r3],-1),Layer_3_2r4],-1)\n",
    "Layer_3_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_4 = False\n",
    "with tf.variable_scope(\"layer_4\",reuse=Reuse_Layer_4) as scope:\n",
    "    \n",
    "    Layer_4 = slim.conv3d(Layer_3_2, 32, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_4 = slim.layer_norm(Layer_4) \n",
    "    Layer_4 = tf.nn.relu(Layer_4)\n",
    "    \n",
    "    Layer_4 = tf.nn.max_pool3d(Layer_4, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_4p')\n",
    "    Layer_4 =  tf.nn.dropout(Layer_4,keep_prob_skip)\n",
    "    print(Layer_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_4 = True\n",
    "with tf.variable_scope(\"layer_4\",reuse=Reuse_Layer_4) as scope:\n",
    "    \n",
    "    Layer_4r2 = slim.conv3d(Layer_3_2, 32, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], rate=3,\n",
    "                          activation_fn=None)\n",
    "    Layer_4r2 = slim.layer_norm(Layer_4r2) \n",
    "    Layer_4r2 = tf.nn.relu(Layer_4r2)\n",
    "    \n",
    "    Layer_4r2 = tf.nn.max_pool3d(Layer_4r2, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_4p')\n",
    "    Layer_4r2 =  tf.nn.dropout(Layer_4r2,keep_prob_skip)\n",
    "    print(Layer_4r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_4 = True\n",
    "with tf.variable_scope(\"layer_4\",reuse=Reuse_Layer_4) as scope:\n",
    "    \n",
    "    Layer_4r3 = slim.conv3d(Layer_3_2, 32, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], rate=7,\n",
    "                          activation_fn=None)\n",
    "    Layer_4r3 = slim.layer_norm(Layer_4r3) \n",
    "    Layer_4r3 = tf.nn.relu(Layer_4r3)\n",
    "    \n",
    "    Layer_4r3 = tf.nn.max_pool3d(Layer_4r3, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_4p')\n",
    "    Layer_4r3 =  tf.nn.dropout(Layer_4r3,keep_prob_skip)\n",
    "    print(Layer_4r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_4 = True\n",
    "with tf.variable_scope(\"layer_4\",reuse=Reuse_Layer_4) as scope:\n",
    "    \n",
    "    Layer_4r4 = slim.conv3d(Layer_3_2, 32, kernel_size=[ 3, 3, 3],stride=[1, 1, 1], rate=9,\n",
    "                          activation_fn=None)\n",
    "    Layer_4r4 = slim.layer_norm(Layer_4r4) \n",
    "    Layer_4r4 = tf.nn.relu(Layer_4r4)\n",
    "    \n",
    "    Layer_4r4 = tf.nn.max_pool3d(Layer_4r4, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_4p')\n",
    "    Layer_4r4 =  tf.nn.dropout(Layer_4r4,keep_prob_skip)\n",
    "    print(Layer_4r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Layer_4 = tf.concat([tf.concat([tf.concat([Layer_4,Layer_4r2],-1),Layer_4r3],-1),Layer_4r4],-1)\n",
    "Layer_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_5 = False\n",
    "with tf.variable_scope(\"layer_5\",reuse=Reuse_Layer_5) as scope:\n",
    "    \n",
    "    Layer_5 = slim.conv3d(Layer_4, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_5 = slim.layer_norm(Layer_5) #slim.batch_norm(net, fused=True)\n",
    "    Layer_5 = tf.nn.relu(Layer_5)\n",
    "    Layer_5 = tf.nn.max_pool3d(Layer_5, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_5p')\n",
    "    Layer_5 =  tf.nn.dropout(Layer_5,keep_prob_skip)\n",
    "    print(Layer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_5 = True\n",
    "with tf.variable_scope(\"layer_5\",reuse=Reuse_Layer_5) as scope:\n",
    "    \n",
    "    Layer_5r2 = slim.conv3d(Layer_4, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=3,\n",
    "                          activation_fn=None)\n",
    "    Layer_5r2 = slim.layer_norm(Layer_5r2) #slim.batch_norm(net, fused=True)\n",
    "    Layer_5r2 = tf.nn.relu(Layer_5r2)\n",
    "    Layer_5r2 = tf.nn.max_pool3d(Layer_5r2, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_5p')\n",
    "    Layer_5r2 =  tf.nn.dropout(Layer_5r2,keep_prob_skip)\n",
    "    print(Layer_5r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_5 = True\n",
    "with tf.variable_scope(\"layer_5\",reuse=Reuse_Layer_5) as scope:\n",
    "    \n",
    "    Layer_5r3 = slim.conv3d(Layer_4, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=7,\n",
    "                          activation_fn=None)\n",
    "    Layer_5r3 = slim.layer_norm(Layer_5r3) #slim.batch_norm(net, fused=True)\n",
    "    Layer_5r3 = tf.nn.relu(Layer_5r3)\n",
    "    Layer_5r3 = tf.nn.max_pool3d(Layer_5r3, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_5p')\n",
    "    Layer_5r3 =  tf.nn.dropout(Layer_5r3,keep_prob_skip)\n",
    "    print(Layer_5r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_5 = True\n",
    "with tf.variable_scope(\"layer_5\",reuse=Reuse_Layer_5) as scope:\n",
    "    \n",
    "    Layer_5r4 = slim.conv3d(Layer_4, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=9,\n",
    "                          activation_fn=None)\n",
    "    Layer_5r4 = slim.layer_norm(Layer_5r4) #slim.batch_norm(net, fused=True)\n",
    "    Layer_5r4 = tf.nn.relu(Layer_5r4)\n",
    "    Layer_5r4 = tf.nn.max_pool3d(Layer_5r4, strides=[1, 1, 2, 2, 1], \n",
    "                                        ksize=[1, 3, 3, 3, 1], padding='SAME', name='Layer_5p')\n",
    "    Layer_5r4 =  tf.nn.dropout(Layer_5r4,keep_prob_skip)\n",
    "    print(Layer_5r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_5 = tf.concat([tf.concat([tf.concat([Layer_5,Layer_5r2],-1),Layer_5r3],-1),Layer_5r4],-1)\n",
    "Layer_5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_6 = False\n",
    "with tf.variable_scope(\"layer_6\",reuse=Reuse_Layer_6) as scope:\n",
    "    \n",
    "    Layer_6_5 = slim.conv3d(Layer_5, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_6_5 = slim.layer_norm(Layer_6_5) #slim.batch_norm(net, fused=True)\n",
    "    Layer_6_5 = tf.nn.relu(Layer_6_5)\n",
    "    Layer_6_5 = unpool_3D(Layer_6_5)\n",
    "    Layer_6_5 =  tf.nn.dropout(Layer_6_5,keep_prob_skip)\n",
    "    print(Layer_6_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_6 = True\n",
    "with tf.variable_scope(\"layer_6\",reuse=Reuse_Layer_6) as scope:\n",
    "    \n",
    "    Layer_6_5r2 = slim.conv3d(Layer_5, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=3,\n",
    "                          activation_fn=None)\n",
    "    Layer_6_5r2 = slim.layer_norm(Layer_6_5r2) #slim.batch_norm(net, fused=True)\n",
    "    Layer_6_5r2 = tf.nn.relu(Layer_6_5r2)\n",
    "    Layer_6_5r2 = unpool_3D(Layer_6_5r2)\n",
    "    Layer_6_5r2 =  tf.nn.dropout(Layer_6_5r2,keep_prob_skip)\n",
    "    print(Layer_6_5r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_6 = True\n",
    "with tf.variable_scope(\"layer_6\",reuse=Reuse_Layer_6) as scope:\n",
    "    \n",
    "    Layer_6_5r3 = slim.conv3d(Layer_5, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=7,\n",
    "                          activation_fn=None)\n",
    "    Layer_6_5r3 = slim.layer_norm(Layer_6_5r3) #slim.batch_norm(net, fused=True)\n",
    "    Layer_6_5r3 = tf.nn.relu(Layer_6_5r3)\n",
    "    Layer_6_5r3 = unpool_3D(Layer_6_5r3)\n",
    "    Layer_6_5r3 =  tf.nn.dropout(Layer_6_5r3,keep_prob_skip)\n",
    "    print(Layer_6_5r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_6 = True\n",
    "with tf.variable_scope(\"layer_6\",reuse=Reuse_Layer_6) as scope:\n",
    "    \n",
    "    Layer_6_5r4 = slim.conv3d(Layer_5, 64, kernel_size=[3, 3, 3],stride=[1, 1, 1], rate=9,\n",
    "                          activation_fn=None)\n",
    "    Layer_6_5r4 = slim.layer_norm(Layer_6_5r4) #slim.batch_norm(net, fused=True)\n",
    "    Layer_6_5r4 = tf.nn.relu(Layer_6_5r4)\n",
    "    Layer_6_5r4 = unpool_3D(Layer_6_5r4)\n",
    "    Layer_6_5r4 =  tf.nn.dropout(Layer_6_5r4,keep_prob_skip)\n",
    "    print(Layer_6_5r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_6_5 = tf.concat([tf.concat([tf.concat([Layer_6_5,Layer_6_5r2],-1),Layer_6_5r3],-1),Layer_6_5r4],-1)\n",
    "Layer_6_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_7 = False\n",
    "with tf.variable_scope(\"layer_7\",reuse=Reuse_Layer_7) as scope:\n",
    "    Layer_7 = slim.conv3d(Layer_6_5, 32, \n",
    "                          kernel_size=[ 3, 3, 3],\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_7 = slim.layer_norm(Layer_7) \n",
    "    Layer_7 = tf.nn.relu(Layer_7)\n",
    "    Layer_7 = unpool_3D(Layer_7)\n",
    "    Layer_7 =  tf.nn.dropout(Layer_7,keep_prob_skip)\n",
    "    print(Layer_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_7 = True\n",
    "with tf.variable_scope(\"layer_7\",reuse=Reuse_Layer_7) as scope:\n",
    "    Layer_7r2 = slim.conv3d(Layer_6_5, 32, \n",
    "                          kernel_size=[ 3, 3, 3],rate=3,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_7r2 = slim.layer_norm(Layer_7r2) \n",
    "    Layer_7r2 = tf.nn.relu(Layer_7r2)\n",
    "    Layer_7r2 = unpool_3D(Layer_7r2)\n",
    "    Layer_7r2 =  tf.nn.dropout(Layer_7r2,keep_prob_skip)\n",
    "    print(Layer_7r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_7 = True\n",
    "with tf.variable_scope(\"layer_7\",reuse=Reuse_Layer_7) as scope:\n",
    "    Layer_7r3 = slim.conv3d(Layer_6_5, 32, \n",
    "                          kernel_size=[ 3, 3, 3],rate=7,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_7r3 = slim.layer_norm(Layer_7r3) \n",
    "    Layer_7r3 = tf.nn.relu(Layer_7r3)\n",
    "    Layer_7r3 = unpool_3D(Layer_7r3)\n",
    "    Layer_7r3 =  tf.nn.dropout(Layer_7r3,keep_prob_skip)\n",
    "    print(Layer_7r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_7 = True\n",
    "with tf.variable_scope(\"layer_7\",reuse=Reuse_Layer_7) as scope:\n",
    "    Layer_7r4 = slim.conv3d(Layer_6_5, 32, \n",
    "                          kernel_size=[ 3, 3, 3], rate=9,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_7r4 = slim.layer_norm(Layer_7r4) \n",
    "    Layer_7r4 = tf.nn.relu(Layer_7r4)\n",
    "    Layer_7r4 = unpool_3D(Layer_7r4)\n",
    "    Layer_7r4 =  tf.nn.dropout(Layer_7r4,keep_prob_skip)\n",
    "    print(Layer_7r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_7 = tf.concat([tf.concat([tf.concat([Layer_7,Layer_7r2],-1),Layer_7r3],-1),Layer_7r4],-1)\n",
    "Layer_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_8 = False\n",
    "with tf.variable_scope(\"layer_8\",reuse=Reuse_Layer_8) as scope:\n",
    "    \n",
    "    Layer_8 = slim.conv3d(Layer_7, 16, \n",
    "                          kernel_size=[ 3, 3, 3],\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_8 = slim.layer_norm(Layer_8) \n",
    "    Layer_8 = tf.nn.relu(Layer_8)\n",
    "    Layer_8 = unpool_3D(Layer_8)\n",
    "    Layer_8 =  tf.nn.dropout(Layer_8,keep_prob_skip)\n",
    "    print(Layer_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_8 = True\n",
    "with tf.variable_scope(\"layer_8\",reuse=Reuse_Layer_8) as scope:\n",
    "    \n",
    "    Layer_8r2 = slim.conv3d(Layer_7, 16, \n",
    "                          kernel_size=[ 3, 3, 3],rate=3,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_8r2 = slim.layer_norm(Layer_8r2) \n",
    "    Layer_8r2 = tf.nn.relu(Layer_8r2)\n",
    "    Layer_8r2 = unpool_3D(Layer_8r2)\n",
    "    Layer_8r2 =  tf.nn.dropout(Layer_8r2,keep_prob_skip)\n",
    "    print(Layer_8r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_8 = True\n",
    "with tf.variable_scope(\"layer_8\",reuse=Reuse_Layer_8) as scope:\n",
    "    \n",
    "    Layer_8r3 = slim.conv3d(Layer_7, 16, \n",
    "                          kernel_size=[ 3, 3, 3],rate=7,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_8r3 = slim.layer_norm(Layer_8r3) \n",
    "    Layer_8r3 = tf.nn.relu(Layer_8r3)\n",
    "    Layer_8r3 = unpool_3D(Layer_8r3)\n",
    "    Layer_8r3 =  tf.nn.dropout(Layer_8r3,keep_prob_skip)\n",
    "    print(Layer_8r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_8 = True\n",
    "with tf.variable_scope(\"layer_8\",reuse=Reuse_Layer_8) as scope:\n",
    "    \n",
    "    Layer_8r4 = slim.conv3d(Layer_7, 16, \n",
    "                          kernel_size=[ 3, 3, 3],rate=9,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_8r4 = slim.layer_norm(Layer_8r4) \n",
    "    Layer_8r4 = tf.nn.relu(Layer_8r4)\n",
    "    Layer_8r4 = unpool_3D(Layer_8r4)\n",
    "    Layer_8r4 =  tf.nn.dropout(Layer_8r4,keep_prob_skip)\n",
    "    print(Layer_8r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_8 = tf.concat([tf.concat([tf.concat([Layer_8,Layer_8r2],-1),Layer_8r3],-1),Layer_8r4],-1)\n",
    "Layer_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_9 = False\n",
    "with tf.variable_scope(\"layer_9\",reuse=Reuse_Layer_9) as scope:\n",
    "    \n",
    "    Layer_9_8 = slim.conv3d(Layer_8, 8, \n",
    "                          kernel_size=[ 3, 3, 3],\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_9_8 = slim.layer_norm(Layer_9_8) \n",
    "    Layer_9_8 = tf.nn.relu(Layer_9_8)\n",
    "    Layer_9_8 = unpool_3D(Layer_9_8)\n",
    "    Layer_9_8 =  tf.nn.dropout(Layer_9_8,keep_prob_skip)\n",
    "    print(Layer_9_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_9 = True\n",
    "with tf.variable_scope(\"layer_9\",reuse=Reuse_Layer_9) as scope:\n",
    "    \n",
    "    Layer_9_8r2 = slim.conv3d(Layer_8, 8, \n",
    "                          kernel_size=[ 3, 3, 3],rate=3,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_9_8r2 = slim.layer_norm(Layer_9_8r2) \n",
    "    Layer_9_8r2 = tf.nn.relu(Layer_9_8r2)\n",
    "    Layer_9_8r2 = unpool_3D(Layer_9_8r2)\n",
    "    Layer_9_8r2 =  tf.nn.dropout(Layer_9_8r2,keep_prob_skip)\n",
    "    print(Layer_9_8r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_9 = True\n",
    "with tf.variable_scope(\"layer_9\",reuse=Reuse_Layer_9) as scope:\n",
    "    \n",
    "    Layer_9_8r3 = slim.conv3d(Layer_8, 8, \n",
    "                          kernel_size=[ 3, 3, 3],rate=7,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_9_8r3 = slim.layer_norm(Layer_9_8r3) \n",
    "    Layer_9_8r3 = tf.nn.relu(Layer_9_8r3)\n",
    "    Layer_9_8r3 = unpool_3D(Layer_9_8r3)\n",
    "    Layer_9_8r3 =  tf.nn.dropout(Layer_9_8r3,keep_prob_skip)\n",
    "    print(Layer_9_8r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_9 = True\n",
    "with tf.variable_scope(\"layer_9\",reuse=Reuse_Layer_9) as scope:\n",
    "    \n",
    "    Layer_9_8r4 = slim.conv3d(Layer_8, 8, \n",
    "                          kernel_size=[ 3, 3, 3],rate=9,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    Layer_9_8r4 = slim.layer_norm(Layer_9_8r4) \n",
    "    Layer_9_8r4 = tf.nn.relu(Layer_9_8r4)\n",
    "    Layer_9_8r4 = unpool_3D(Layer_9_8r4)\n",
    "    Layer_9_8r4 =  tf.nn.dropout(Layer_9_8r4,keep_prob_skip)\n",
    "    print(Layer_9_8r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_9_8 = tf.concat([tf.concat([tf.concat([Layer_9_8,Layer_9_8r2],-1),Layer_9_8r3],-1),Layer_9_8r4],-1)\n",
    "Layer_9_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_9_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_10 = False\n",
    "with tf.variable_scope(\"layer_10\",reuse=Reuse_Layer_10) as scope:\n",
    "    \n",
    "    Layer_10 = slim.conv3d(Layer_9_8, 4, \n",
    "                          kernel_size=[ 3, 3, 3],\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_10 = slim.layer_norm(Layer_10) \n",
    "    Layer_10 = tf.nn.relu(Layer_10)\n",
    "\n",
    "    Layer_10 = unpool_3D(Layer_10)\n",
    "    Layer_10 =  tf.nn.dropout(Layer_10,keep_prob_skip)\n",
    "    \n",
    "    print(Layer_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_10 = True\n",
    "with tf.variable_scope(\"layer_10\",reuse=Reuse_Layer_10) as scope:\n",
    "    \n",
    "    Layer_10r2 = slim.conv3d(Layer_9_8, 4, \n",
    "                          kernel_size=[ 3, 3, 3],rate=3,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_10r2 = slim.layer_norm(Layer_10r2) \n",
    "    Layer_10r2 = tf.nn.relu(Layer_10r2)\n",
    "\n",
    "    Layer_10r2 = unpool_3D(Layer_10r2)\n",
    "    Layer_10r2 =  tf.nn.dropout(Layer_10r2,keep_prob_skip)\n",
    "    \n",
    "    print(Layer_10r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_10 = True\n",
    "with tf.variable_scope(\"layer_10\",reuse=Reuse_Layer_10) as scope:\n",
    "    \n",
    "    Layer_10r3 = slim.conv3d(Layer_9_8, 4, \n",
    "                          kernel_size=[ 3, 3, 3],rate=7,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_10r3 = slim.layer_norm(Layer_10r3) \n",
    "    Layer_10r3 = tf.nn.relu(Layer_10r3)\n",
    "\n",
    "    Layer_10r3 = unpool_3D(Layer_10r3)\n",
    "    Layer_10r3 =  tf.nn.dropout(Layer_10r3,keep_prob_skip)\n",
    "    \n",
    "    print(Layer_10r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_Layer_10 = True\n",
    "with tf.variable_scope(\"layer_10\",reuse=Reuse_Layer_10) as scope:\n",
    "    \n",
    "    Layer_10r4 = slim.conv3d(Layer_9_8, 4, \n",
    "                          kernel_size=[ 3, 3, 3],rate=9,\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    \n",
    "    Layer_10r4 = slim.layer_norm(Layer_10r4) \n",
    "    Layer_10r4 = tf.nn.relu(Layer_10r4)\n",
    "\n",
    "    Layer_10r4 = unpool_3D(Layer_10r4)\n",
    "    Layer_10r4 =  tf.nn.dropout(Layer_10r4,keep_prob_skip)\n",
    "    \n",
    "    print(Layer_10r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_10 = tf.concat([tf.concat([tf.concat([Layer_10,Layer_10r2],-1),Layer_10r3],-1),Layer_10r4],-1)\n",
    "Layer_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_logits = False\n",
    "with tf.variable_scope(\"Logits\",reuse=Reuse_logits) as scope:\n",
    "    \n",
    "    logits = slim.conv3d(Layer_10, 2, \n",
    "                          kernel_size=[ 3, 3, 3],\n",
    "                          stride=[1, 1, 1], \n",
    "                          activation_fn=None)\n",
    "    logits = tf.nn.relu(logits)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define for 5D Tensor, here we are dealing with 5D: axis=3 become axis=4 last channel\n",
    "def pixel_wise_softmax(output_map):\n",
    "    return tf.nn.softmax(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuseflagOutputHM= False\n",
    "with tf.variable_scope(\"output\",reuse=reuseflagOutputHM) as scope:\n",
    "    Segmentation_Maps = pixel_wise_softmax(logits)\n",
    "    print(Segmentation_Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Loss based on Tversky Index and Learned Alpha and Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with pseduo Code which is our previous estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth=1\n",
    "P_foreground_SOL = Segmentation_Maps[...,:1]\n",
    "P_background_SOL  = Segmentation_Maps[...,1:]\n",
    "g_foreground_SOL  = y[...,:1]\n",
    "g_background_SOL  = y[...,1:]\n",
    "print(P_foreground_SOL)\n",
    "print(P_background_SOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the TP, FP, and FN for each Muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOL\n",
    "true_positive_SOL= P_foreground_SOL * g_foreground_SOL\n",
    "#P_foreground_SOL = tf.nn.dropout(P_foreground_SOL,keep_prob_tp,name=\"P_foreground_drop_out_SOL\")\n",
    "false_pos_SOL = P_foreground_SOL * g_background_SOL\n",
    "false_neg_SOL = P_background_SOL * g_foreground_SOL\n",
    "\n",
    "print(true_positive_SOL)\n",
    "print(false_pos_SOL)\n",
    "print(false_neg_SOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable2(shape):\n",
    "    weights_initializer=tf.random_uniform_initializer(minval=0.5, maxval=0.5, seed=None)\n",
    "    Weight = tf.Variable(weights_initializer(shape=shape))    \n",
    "    return Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_tanh = weight_variable2([1, 2])\n",
    "W_tanh = tf.nn.softmax(W_tanh)\n",
    "print(W_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penealise the FP and the FN by learned alpha and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_SOL=tf.reduce_sum(tf.reduce_sum(true_positive_SOL, axis=(2,3,4)),axis=(1))\n",
    "false_pos_SOL=tf.reduce_sum(W_tanh[0][0]*tf.reduce_sum(false_pos_SOL, axis=(2,3,4)),axis=(1))\n",
    "false_neg_SOL=tf.reduce_sum(W_tanh[0][1]*tf.reduce_sum(false_neg_SOL, axis=(2,3,4)),axis=(1))\n",
    "print(true_positive_SOL)\n",
    "print(false_pos_SOL)\n",
    "print(false_neg_SOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Tversky Index for each of the muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TverskyIndex_SOL = tf.divide(true_positive_SOL+smooth, (true_positive_SOL + false_pos_SOL  + false_neg_SOL  + smooth))\n",
    "print(TverskyIndex_SOL)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    parametricTverskyLoss = 1-tf.reduce_mean(TverskyIndex_SOL)\n",
    "print(parametricTverskyLoss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schaduale Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schaduale Learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "boundaries = [6000]\n",
    "values = [0.0001,0.00001]\n",
    "learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(parametricTverskyLoss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Some Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dice Value\n",
    "IoU_SOL = iou_coe(Segmentation_Maps,y)      \n",
    "print(IoU_SOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dice Value\n",
    "TverskyIndexValue_SOL = Tversky_Index(Segmentation_Maps,y)      \n",
    "print(TverskyIndexValue_SOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Precision_Recall\"):\n",
    "    output_thres_SOL = tf.cast(tf.squeeze(Segmentation_Maps[...,:1],0)> 0.5, dtype=tf.float32)\n",
    "    \n",
    "    target_thres_SOL = tf.cast(tf.squeeze(y[...,:1],0)> 0.5, dtype=tf.float32)\n",
    "    \n",
    "    precVSOL, recallVSOL = confusion(target_thres_SOL,output_thres_SOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MeanP = tf.reduce_mean(precVSOL+precVGL+precVGM)\n",
    "#MeanR = tf.reduce_mean(recallVSOL+recallVGL+recallVGM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "model_path = '/tf/2021Work/AnalysisiFilterSharingV2_3/Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_o = tf.global_variables_initializer()\n",
    "Modelsummary={}\n",
    "isRestor=False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import random # to generate a random number for dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_iou(pred, label):\n",
    "\n",
    "    unique_labels = np.unique(label)\n",
    "    num_unique_labels = len(unique_labels);\n",
    "\n",
    "    I = np.zeros(num_unique_labels)\n",
    "    U = np.zeros(num_unique_labels)\n",
    "\n",
    "    for index, val in enumerate(unique_labels):\n",
    "        pred_i = pred == val\n",
    "        label_i = label == val\n",
    "\n",
    "        I[index] = float(np.sum(np.logical_and(label_i, pred_i)))\n",
    "        U[index] = float(np.sum(np.logical_or(label_i, pred_i)))\n",
    "    mean_iou = np.mean(I / U)\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "incremental learning over the subjects with 50% 40 30 20 10 ...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use the 10 Patient as Training from the Test Set and the 1st 19 Patients from the training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_path = '/tf/volumes/test/'\n",
    "Patient_folder_path = sorted(os.listdir(x_y_path))\n",
    "Patient_folder_full_path = list(map(lambda v : str(join(x_y_path,v)) + '/', Patient_folder_path))\n",
    "\n",
    "# Get the full path for the volume and the mask\n",
    "DataTestPath = list(map(lambda s : s+'x1.mha' , Patient_folder_full_path))\n",
    "MasksTestPath = list(map(lambda s : s+'masksX1.mha' , Patient_folder_full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ValidationSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path \n",
    "x_y_path = '/tf/volumes/val/'\n",
    "# define the full path for each patient\n",
    "Patient_folder_path = sorted(os.listdir(x_y_path))\n",
    "Patient_folder_full_path = list(map(lambda v : str(join(x_y_path,v)) + '/', Patient_folder_path))\n",
    "\n",
    "# Get the full path for the volume and the mask\n",
    "DataValPath = list(map(lambda s : s+'x1.mha' , Patient_folder_full_path))\n",
    "MasksValPath = list(map(lambda s : s+'masksX1.mha' , Patient_folder_full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TrainSetDataPath = DataTestPath+DataTrainPath[:19]\n",
    "TrainSetMaskPath = MasksTestPath+MasksTrainPath[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSetDataPath = DataTestPath+DataTrainPath+DataValPath\n",
    "TrainSetMaskPath = MasksTestPath+MasksTrainPath+MasksValPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TrainSetDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReturnIndicesOfFullMask(sequence):\n",
    "    '''\n",
    "    The input sequence is of shape (1583, 512, 512)\n",
    "    The output is a list of 2 indices, the begining and the end of thesequence with full masks\n",
    "    Return the begining and the end of the sequence\n",
    "    '''\n",
    "    result = list(map(lambda img: img.sum() ,sequence ))\n",
    "    resultIndex = list(map(lambda element: element>2000 ,result))\n",
    "    Indices = [i for i, x in enumerate(resultIndex) if x]\n",
    "    return Indices[0],Indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SampleData(subjects): \n",
    "    StartEnd =list(map(lambda el1: ReturnIndicesOfFullMask(process_mask_GL(TrainSetMaskPath[el1])[:,:,:,0]),\n",
    "                              subjects))\n",
    "    indicesToSampleFrom = list(map(lambda el1:random.sample(range(el1[0],el1[1]-3), 1)[0], StartEnd))\n",
    "    SampledIndicies = list(map(lambda el1:list(range(el1,el1+3)), indicesToSampleFrom))\n",
    "    x_train = np.array(list(map(lambda a,b:Pull_data_from_path(TrainSetDataPath[a])[b],subjects,SampledIndicies)))\n",
    "    y_train = np.array(list(map(lambda a,b:process_mask_GL(TrainSetMaskPath[a])[b],subjects,SampledIndicies)))\n",
    "    return x_train,y_train\n",
    "\n",
    "def Binaize(im):\n",
    "    return cv2.threshold(im[...,0],0.5,1,cv2.THRESH_BINARY)[1].astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3,figsize=(12,6))\n",
    "fig.tight_layout(pad=1.5)\n",
    "\n",
    "isRestor=False\n",
    "file_path = model_path + 'model-GL'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if isRestor:\n",
    "        sess.run(init_o) \n",
    "        saver.restore(sess,file_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(init_o) \n",
    "        print('Model Training from Scratch')\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    \n",
    "    itCounter=0\n",
    "    \n",
    "    train_Dice_Coef_list=[] \n",
    "    train_loss_list=[]      \n",
    "    train_mIoU_list=[] \n",
    "    train_P_list=[] \n",
    "    train_R_list=[] \n",
    "    train_alpha_list = []\n",
    "    train_beta_list = []\n",
    "        \n",
    "    batchsize=2\n",
    "    for iterations in range(3001):\n",
    "        SubjectToSampleFrom = random.sample(range(0,29), batchsize) # 3 mean batch of 8\n",
    "        x_train,y_train =  SampleData(SubjectToSampleFrom)\n",
    "        print('Input Shape: ', x_train.shape)\n",
    "\n",
    "        # Feed the network to train with the Reference and the Target\n",
    "        _, dice_losst,current_output,alpha,betta=sess.run([train_step,parametricTverskyLoss,\n",
    "                                                                  Segmentation_Maps,W_tanh[0][0],W_tanh[0][1]],\n",
    "                                                                 feed_dict={\n",
    "                                                                     x: x_train,\n",
    "                                                                     y: y_train,\n",
    "                                                                     keep_prob:random.uniform(0.85, 1.),\n",
    "                                                keep_prob_input:random.uniform(0.9, 1.),\n",
    "                                                keep_prob_skip:random.uniform(0.85, 1.),\n",
    "                                                keep_prob_alpha :1.0,\n",
    "                                                keep_prob_beta :1.0,\n",
    "                                                keep_prob_tp:0.95\n",
    "                                               })\n",
    "            \n",
    "        im1 = current_output[0][-1][...,0].reshape(512,512)    \n",
    "        \n",
    "         # append the values over T segment and compute their mean for each patient   \n",
    "        IOU = metric.jc(np.array(list(map(lambda a:list(map(lambda b:Binaize(b), a)), current_output))), y_train[...,0])\n",
    "        DICE =  metric.dc(np.array(list(map(lambda a:list(map(lambda b:Binaize(b), a)), current_output))), y_train[...,0])\n",
    "        Precision = metric.precision(np.array(list(map(lambda a:list(map(lambda b:Binaize(b), a)), current_output))), y_train[...,0])\n",
    "        Recall = metric.recall(np.array(list(map(lambda a:list(map(lambda b:Binaize(b), a)), current_output))), y_train[...,0])\n",
    "\n",
    "        print('compute the mean of the results')\n",
    "            \n",
    "        axs[0, 0].set_title('Tversky loss')\n",
    "        axs[0, 1].set_title('mIoU')\n",
    "        axs[0, 2].set_title('Dice Score')\n",
    "\n",
    "        axs[1, 0].set_title('Alpha and Beta')\n",
    "            \n",
    "        axs[1, 1].set_title('P-R')\n",
    "        axs[1, 2].set_title('Heat Map')\n",
    "            \n",
    "        axs[0, 0].plot([itCounter],dice_losst,'b*')\n",
    "        axs[0, 1].plot([itCounter],IOU,'b*')\n",
    "        axs[0, 2].plot([itCounter],DICE,'k*')                           \n",
    "                                                \n",
    "        axs[1, 0].plot([itCounter],alpha,'b*')\n",
    "        axs[1, 0].plot([itCounter],betta,'g*')\n",
    "            \n",
    "        axs[1, 1].plot([itCounter],Precision,'b*')\n",
    "        axs[1, 1].plot([itCounter],Recall,'k*')\n",
    "\n",
    "        axs[1, 2].imshow(im1, aspect=\"auto\",cmap='gist_earth')\n",
    "            \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())   \n",
    "        sys.stdout.flush()\n",
    "            \n",
    "        Modelsummary[itCounter]=[dice_losst, IOU,DICE,Precision,Recall,alpha,betta]  \n",
    "            \n",
    "        itCounter=itCounter+1\n",
    "\n",
    "        print('next subject')\n",
    "        \n",
    "        if iterations%200==0:\n",
    "            print('>> Model Saved and can be restored during another session!')\n",
    "            saver.save(sess, file_path, global_step=iterations) \n",
    "            \n",
    "            with open(model_path+'learningCurve.pickle', 'wb') as handle:\n",
    "                    pickle.dump(Modelsummary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('>> Model Saved and can be restored during another session!')\n",
    "    saver.save(sess, file_path, global_step=iterations) \n",
    "            \n",
    "    with open(model_path+'learningCurve.pickle', 'wb') as handle:\n",
    "        pickle.dump(Modelsummary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
